#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –∏–∑ notebook –≤ —Ñ–∞–π–ª
"""
import sys
import os

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –¥–ª—è –∏–º–ø–æ—Ä—Ç–∞
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

import json
import joblib
import pandas as pd
import numpy as np
from sklearn.preprocessing import RobustScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.ensemble import RandomForestClassifier

print("="*80)
print("–°–û–•–†–ê–ù–ï–ù–ò–ï –ú–û–î–ï–õ–ò –î–õ–Ø STREAMLIT –ü–†–ò–õ–û–ñ–ï–ù–ò–Ø")
print("="*80)

# –ü—ã—Ç–∞–µ–º—Å—è –∑–∞–≥—Ä—É–∑–∏—Ç—å –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –∏–∑ notebook —á–µ—Ä–µ–∑ pickle/IPython
# –ò–ª–∏ —Å–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö

# –ó–∞–≥—Ä—É–∂–∞–µ–º –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã
if os.path.exists('best_hyperparameters.json'):
    with open('best_hyperparameters.json', 'r', encoding='utf-8') as f:
        hyperparams = json.load(f)
    
    print(f"\n‚úÖ –ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∑–∞–≥—Ä—É–∂–µ–Ω—ã –∏–∑ best_hyperparameters.json")
    print(f"   –ú–æ–¥–µ–ª—å: {hyperparams.get('model', 'Unknown')}")
    print(f"   F1-score: {hyperparams.get('best_score', 0):.4f}")
else:
    print("‚ùå –§–∞–π–ª best_hyperparameters.json –Ω–µ –Ω–∞–π–¥–µ–Ω!")
    sys.exit(1)

# –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
data_path = "data"
if not os.path.exists(data_path):
    print(f"‚ùå –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å –¥–∞–Ω–Ω—ã–º–∏ '{data_path}' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞!")
    print("   –ù–µ–≤–æ–∑–º–æ–∂–Ω–æ —Å–æ–∑–¥–∞—Ç—å –º–æ–¥–µ–ª—å –±–µ–∑ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è.")
    sys.exit(1)

print(f"\nüìä –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ {data_path}...")

# –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ (–ø—Ä–æ—Å—Ç–∞—è –≤–µ—Ä—Å–∏—è - –±–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ –¥–æ—Å—Ç—É–ø–Ω—ã–µ —Ñ–∞–π–ª—ã)
try:
    csv_files = [f for f in os.listdir(data_path) if f.endswith('.csv')]
    if not csv_files:
        print("‚ùå CSV —Ñ–∞–π–ª—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã!")
        sys.exit(1)
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –æ–¥–∏–Ω —Ñ–∞–π–ª –¥–ª—è –æ–±—É—á–µ–Ω–∏—è (–∏–ª–∏ –º–æ–∂–Ω–æ –æ–±—ä–µ–¥–∏–Ω–∏—Ç—å –≤—Å–µ)
    # –ó–¥–µ—Å—å –∏—Å–ø–æ–ª—å–∑—É–µ–º —É–ø—Ä–æ—â–µ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ - –∑–∞–≥—Ä—É–∂–∞–µ–º –ø–µ—Ä–≤—ã–π —Ñ–∞–π–ª —Å –¥–∞–Ω–Ω—ã–º–∏
    print(f"   –ù–∞–π–¥–µ–Ω–æ {len(csv_files)} CSV —Ñ–∞–π–ª–æ–≤")
    
    # –ü—Ä–æ–±—É–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –∏–∑ —Ñ–∞–π–ª–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ —Å–æ–¥–µ—Ä–∂–∞—Ç –Ω—É–∂–Ω—ã–µ —Å—Ç–æ–ª–±—Ü—ã
    required_cols = ['GR', 'RHOB', 'NPHI', 'RDEP', 'FORCE_2020_LITHOFACIES_LITHOLOGY']
    
    all_data = []
    for file in csv_files[:5]:  # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 5 —Ñ–∞–π–ª–æ–≤
        try:
            file_path = os.path.join(data_path, file)
            df = pd.read_csv(file_path)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö —Å—Ç–æ–ª–±—Ü–æ–≤
            if all(col in df.columns for col in required_cols):
                # –î–æ–±–∞–≤–ª—è–µ–º WELL_NAME –µ—Å–ª–∏ –µ–≥–æ –Ω–µ—Ç
                if 'WELL_NAME' not in df.columns:
                    df['WELL_NAME'] = file.replace('.csv', '')
                all_data.append(df[required_cols + ['WELL_NAME']])
        except Exception as e:
            continue
    
    if not all_data:
        print("‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω—ã —Ñ–∞–π–ª—ã —Å –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–º–∏ —Å—Ç–æ–ª–±—Ü–∞–º–∏!")
        sys.exit(1)
    
    # –û–±—ä–µ–¥–∏–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ
    combined_df = pd.concat(all_data, ignore_index=True)
    print(f"   ‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(combined_df)} –∑–∞–ø–∏—Å–µ–π")
    
    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
    if 'FORCE_2020_LITHOFACIES_LITHOLOGY' in combined_df.columns:
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Ü–µ–ª–µ–≤—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        if combined_df['FORCE_2020_LITHOFACIES_LITHOLOGY'].dtype == 'object':
            from sklearn.preprocessing import LabelEncoder
            le = LabelEncoder()
            combined_df['FORCE_2020_LITHOFACIES_LITHOLOGY'] = le.fit_transform(
                combined_df['FORCE_2020_LITHOFACIES_LITHOLOGY']
            )
    
    # –£–¥–∞–ª—è–µ–º –ø—Ä–æ–ø—É—Å–∫–∏ (–∏–ª–∏ –º–æ–∂–Ω–æ –∑–∞–ø–æ–ª–Ω–∏—Ç—å)
    combined_df = combined_df.dropna()
    
    if len(combined_df) < 100:
        print(f"‚ö†Ô∏è –ú–∞–ª–æ –¥–∞–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {len(combined_df)} –∑–∞–ø–∏—Å–µ–π")
    
    # –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ X –∏ y
    X = combined_df[['GR', 'RHOB', 'NPHI', 'RDEP', 'WELL_NAME']]
    y = combined_df['FORCE_2020_LITHOFACIES_LITHOLOGY']
    
    print(f"\nüî® –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–º–∏ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏...")
    
    # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    params = hyperparams.get('best_params', {})
    
    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    model_params = {}
    for key, value in params.items():
        if value == "None":
            model_params[key] = None
        elif value == "True":
            model_params[key] = True
        elif value == "False":
            model_params[key] = False
        elif key == 'n_estimators':
            model_params[key] = int(value)
        elif key in ['min_samples_split', 'min_samples_leaf']:
            model_params[key] = int(value)
        elif key == 'max_features':
            if value in ['sqrt', 'log2']:
                model_params[key] = value
            else:
                model_params[key] = float(value)
        elif key == 'max_depth':
            if value == "None":
                model_params[key] = None
            else:
                model_params[key] = int(value) if str(value).isdigit() else None
        else:
            model_params[key] = value
    
    # –°–æ–∑–¥–∞–µ–º pipeline
    numeric_features = ['GR', 'RHOB', 'NPHI', 'RDEP']
    categorical_features = ['WELL_NAME']
    
    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º WELL_NAME –≤ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–π —Ç–∏–ø
    X['WELL_NAME'] = X['WELL_NAME'].astype('category')
    
    preprocessor = ColumnTransformer(
        transformers=[
            ('num', RobustScaler(), numeric_features),
            ('cat', OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore'), categorical_features)
        ],
        remainder='passthrough'
    )
    
    # –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å —Å –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
    rf_model = RandomForestClassifier(
        random_state=42,
        n_jobs=-1,
        **model_params
    )
    
    final_pipeline = Pipeline([
        ('preprocessor', preprocessor),
        ('classifier', rf_model)
    ])
    
    print(f"   –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏:")
    for param, value in model_params.items():
        print(f"      - {param}: {value}")
    
    print(f"\n‚è≥ –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –Ω–∞ {len(X)} –∑–∞–ø–∏—Å—è—Ö...")
    final_pipeline.fit(X, y)
    
    print(f"‚úÖ –ú–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞!")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
    model_filename = 'best_pipeline_final.pkl'
    joblib.dump(final_pipeline, model_filename)
    
    print(f"\n‚úÖ –ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ —Ñ–∞–π–ª: {model_filename}")
    print(f"   –†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞: {os.path.getsize(model_filename) / (1024*1024):.2f} MB")
    
    print(f"\nüí° –¢–µ–ø–µ—Ä—å –º–æ–∂–Ω–æ –ø–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç—å Streamlit –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ:")
    print(f"   streamlit run streamlit_app.py")
    
except Exception as e:
    print(f"\n‚ùå –û—à–∏–±–∫–∞: {str(e)}")
    import traceback
    traceback.print_exc()
    sys.exit(1)

